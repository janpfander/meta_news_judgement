# Moderators {#moderators}

\FloatBarrier

All moderator analyses, with the exception of political concordance, only reveal statistical associations, not causal effects, because the moderator variables vary mostly between studies: For example, some studies provided news sources, while others did not. But these studies differ in many other ways, all of which potentially confound any observed association.

Table \@ref(tab:moderators-discernment-table) shows the results of the different meta regressions by moderator variable on discernment and Table \@ref(tab:moderators-bias-table) on skepticism bias. Figures \@ref(fig:moderators-discernment) and \@ref(fig:moderators-bias) visualize those results by showing the distribution of effect sizes by moderator variable.

```{r moderators-discernment-table}
# display models
modelsummary::modelsummary(moderator_models_accuracy,
                           title = 'Moderator effects on Discernment',
                           #stars = TRUE,
                           coef_rename = c("country_groupedUS" = "Country: US (vs. nonUS)",
                                           "political_concordancediscordant" = "Political Concordance : Discordant (vs. Concordant)",
                                           "news_family_groupedother" = "News family: Other (vs. Covid)",
                                           "news_family_groupedpolitical" = "News family: Political (vs. Covid)",
                                           "news_format_groupedheadline_picture" = "News Format: Headline & Picture (vs. Headline)",
                                           "news_format_groupedheadline_picture_lede" = "News Format: Headline, Picture & Lede (vs. Headline)",
                                           "news_sourceTRUE" = "News source: Source (vs. No source)",
                                           "accuracy_scale_grouped6" = "Accuracy Scale: 6 (vs. 4)",
                                           "accuracy_scale_grouped7" = "Accuracy Scale: 7 (vs. 4)",
                                           "accuracy_scale_groupedbinary" = "Accuracy Scale: binary (vs. 4)",
                                           "accuracy_scale_groupedother" = "Accuracy Scale: other (vs. 4)",
                                           "perfect_symetryTRUE" = "Symmetrie: perfect (vs. imperfect)",
                                           "selection_fake_news_groupedidentified by researchers" = "False news: verified by researchers (vs. taken from fact check sites)"
                                           ), 
                           estimate = "{estimate}",
                           statistic = c("z = {statistic}",
                                         "p = {p.value}"),
                           output = "kableExtra"  # Ensure kableExtra is used
                           ) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down") %>%
  footnote(general = "Results of meta-regressions for different moderator variables. No adjustements have been made."
           , threeparttable = TRUE)
```

```{r moderators-bias-table}
# display models
modelsummary::modelsummary(moderator_models_error,
                           title = 'Moderator effects on Skepticism  bias',
                           #stars = TRUE,
                           coef_rename = c("country_groupedUS" = "Country: US (vs. nonUS)",
                                           "political_concordancediscordant" = "Political Concordance : Discordant (vs. Concordant)",
                                           "news_family_groupedother" = "News family: Other (vs. Covid)",
                                           "news_family_groupedpolitical" = "News family: Political (vs. Covid)",
                                           "news_format_groupedheadline_picture" = "News Format: Headline & Picture (vs. Headline)",
                                           "news_format_groupedheadline_picture_lede" = "News Format: Headline, Picture & Lede (vs. Headline)",
                                           "news_sourceTRUE" = "News source: Source (vs. No source)",
                                           "accuracy_scale_grouped6" = "Accuracy Scale: 6 (vs. 4)",
                                           "accuracy_scale_grouped7" = "Accuracy Scale: 7 (vs. 4)",
                                           "accuracy_scale_groupedbinary" = "Accuracy Scale: binary (vs. 4)",
                                           "accuracy_scale_groupedother" = "Accuracy Scale: other (vs. 4)",
                                           "perfect_symetryTRUE" = "Symmetrie: perfect (vs. imperfect)",
                                           "selection_fake_news_groupedidentified by researchers" = "False news: verified by researchers (vs. taken from fact check sites)"
                                           ), 
                           estimate = "{estimate}",
                           statistic = c("z = {statistic}",
                                         "p = {p.value}"),
                           output = "kableExtra"  # Ensure kableExtra is used
                           ) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down") %>%
  footnote(general = "Results of meta-regressions for different moderator variables. No adjustements have been made."
           , threeparttable = TRUE)
```

(ref:moderators-discernment) *Moderator effects on discernment*. The figure shows the distribution of effect sizes for discernment by moderator variables.

```{r moderators-discernment, fig.height= 10, fig.cap="(ref:moderators-discernment)"}
moderator_plots(accuracy_effect, "Discernment")
```

(ref:moderators-bias) *Moderator effects on skepticism bias*. The figure shows the distribution of effect sizes for skepticism bias by moderator variables.

```{r moderators-bias, fig.height= 10, fig.cap="(ref:moderators-bias)"}
moderator_plots(error_effect, "Skepticism  bias")
```

### Not preregistered moderators

#### Scale symmetry

First, to avoid biasing our estimate for H2, we removed one study [@baptistaInfluencePoliticalIdeology2021] that used a very asymmetrical set of answer options asked participants ("According to your knowledge, how do you rate the following headline? 1---not credible; 2---somehow credible; 3---quite credible; 4---credible; 5---very credible"). Second, we coded whether the remaining scales were perfectly symmetrical or not. Table \@ref(tab:n-symmetry) shows the frequency by which both scale types occurred.

```{r n-symmetry}
meta_wide %>% group_by(perfect_symetry) %>% summarise(Papers = n_distinct(paperID),
                                                     Samples = n_distinct(unique_sample_id),
                                                     Effects = n_distinct(observation_id)
                                                     ) %>% 
  mutate(perfect_symetry = ifelse(perfect_symetry == TRUE, "Perfect symmetry", 
                                 "Imperfect Symmetry")) %>% 
  pivot_longer(-perfect_symetry,
               names_to = "variable",
               values_to = "frequency"
               ) %>% 
  pivot_wider(names_from = perfect_symetry, 
              values_from = frequency) %>% 
  column_to_rownames(var = "variable") %>% 
  apa_table(caption = "Frequency table of scales")
```

Perfectly symmetrical scales include all binary scales (e.g. "True" or "False", "Real" or "Fake", is accurate "Yes" or "No", is accurate and unbiased "Yes" or "No"), and most Likert-scales (1 to 7: "Definitely fake" [1] to "Definitely real" [7], "Very unreliable" [1] to "Very reliable" [7], "Extremely unlikely" [1] to "Extremely likely" [7], "Extremely unbelievable" [1] to "Extremely believable" [7]; 1 to 6: "Extremely inaccurate" [1] to "Extremely accurate" [6], "Completely false" [1] to "Completely true" [6]). Yet, we coded the most common scale, a 4-point Likert scale ([1] not at all accurate, [2] not very accurate, [3] somewhat accurate, [4] very accurate), as not perfectly symmetrical. We coded two other Likert scales as not perfectly symmetrical ("not at all trustworthy" [1] to "very trustworthy" [10]; "not at all" [1] to "very" [7]).

Third, we investigated whether H1 and H2 hold for both perfectly symmetrical and imperfectly symmetrical scales. While both H1 and H2 hold for both symmetry types, we found that studies with perfectly symmetric scales tend to yield lower discernment scores ($\Delta$ Discernment = `r list_moderators_accuracy$Symmetrie$perfect_symetryTRUE$estimate` `r list_moderators_accuracy$Symmetrie$perfect_symetryTRUE$ci`) than studies relying on scales that are at least slightly asymmetric (Baseline discernment slightly asymmetric scales = `r list_moderators_accuracy$Symmetrie$intercept$estimate` `r list_moderators_accuracy$Symmetrie$intercept$ci`). Importantly, we do not find a difference regarding skepticism bias.

The results suggest that imperfectly symmetrical scales may inflate discernment. However, the symmetry of response scales was not a factor that was experimentally manipulated, and the studies we compare in our model differ in many other ways and the observed difference is likely confounded.

#### Proportion of true news

Most studies exposed participants to 50% of false news and 50% of true news, whereas outside of experimental settings, people on average are exposed to much more true news than false news [@altayQuantifyingInfodemicPeople2022]. This inflated proportion of false news may increase discernment or make participants more skeptical of true news. Empirical evidence suggests that the ratio of false news has no effect on discernment and slightly increases skepticism in news judgment [@altayExposureHigherRates2023]. Figure \@ref(fig:share-true) shows effect sizes for discernment and skepticism bias as a function of news ratio. Due to the very uneven number of effect sizes, it does not seem reasonable to run a meta-regression to test this. However, Fig. \@ref(fig:share-true) suggests no obvious trend with regard to the share of true news ratio. Besides, as for the other moderator variables, any observed association is likely to be confounded by other factors.

(ref:share-true) *Effects of true-false ratio*. Effect sizes plotted by their share of true news among all news that an individual participant saw.

```{r share-true, fig.cap="(ref:share-true)"}
# for a plot with share as a factor variable, run this:
# share_true_discernment <- plot_share_true(data = accuracy_effect, name = "Discernment")
# share_true_bias <- plot_share_true(data = error_effect, name = "Skepticism  bias")

share_true_discernment <- plot_share_true_continuous(data = accuracy_effect, name = "Discernment")

share_true_bias <- plot_share_true_continuous(data = error_effect, name = "Skepticism  bias")

ggarrange(share_true_discernment, 
          share_true_bias  + 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank()))
```

#### Selection of false news

The majority of studies selected false news items from fact checking sites (e.g. Snopes). However, in three studies included in our meta-analysis, researchers automatically sampled news items and hired fact-checkers to establish their veracity, or did it themselves [@garrettConservativesSusceptibilityPolitical2021; @aslettOnlineSearchesEvaluate2024; @allenScalingFactcheckingUsing2021]. As shown in Tables \@ref(tab:moderators-discernment-table) we find no difference in discernment when comparing news fact checked by fact checking sites compared to news fact checked by researchers. We do find a difference regarding skepticism bias (see Table \@ref(tab:moderators-bias-table)), such that the news items fact checked by researchers show no skepticism bias. 

Note that, as with all between-study moderators, these estimates are likely confounded. The vast majority of effect sizes in the 'verified by researchers' category come from a single panel study [@garrettConservativesSusceptibilityPolitical2021]. This paper finds a negative skepticism bias for politically concordant news, suggesting that people are gullible towards information they politically approve. They did not find a skepticism bias for politically discordant items. Political concordance, therefore, is one reasonable candidate of a confounder for the observed difference regarding false news selection. However, as shown in Appendix \@ref(selection-bias), the (comparatively few) effect sizes from the other two studies relying on automated news selection also consistently yield a negative skepticism bias (i.e. gullibility bias). 

```{r}
# Code to check results from the Garrett study
# Garrett <- meta_wide %>% filter(ref == "Garrett_2021")
# 
# # calculate main effect sizes (Cohen's d, following Cochrane) 
# Garrett_accuracy_effect <- calculate_effect_sizes(effect = "accuracy", measure = "Cochrane", data = Garrett)
# Garrett_error_effect <- calculate_effect_sizes(effect = "error", measure = "Cochrane", data = Garrett)
# 
# # Models using Cohen's d
# Garrett_model_accuracy <- calculate_models(data=Garrett_accuracy_effect, robust = TRUE)
# Garrett_model_error <- calculate_models(data=Garrett_error_effect, robust = TRUE)
# 
# # Model for skepticism bias with concordance as moderator
# Garrett_error_concordance <-  metafor::rma.mv(yi, vi, 
#                                                 mods = ~political_concordance,
#                                                 random = ~ 1 | unique_sample_id/observation_id, 
#                                                 data=Garrett_error_effect)
# 
# summary(Garrett_model_accuracy)
# summary(Garrett_model_error)
# summary(Garrett_error_concordance)
```

