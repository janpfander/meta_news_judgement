# Moderators {#moderators}

\FloatBarrier

All moderator analyses, with the exception of political concordance, only reveal statistical associations, not causal effects, because the moderator variables vary mostly between studies: For example, some studies provided news sources, while others did not. But these studies differ in many other ways, all of which potentially confound any observed association.

Table \@ref(tab:moderators-discernment-table) shows the results of the different meta regressions by moderator variable on discernment and Table \@ref(tab:moderators-bias-table) on skepticism bias. Figures \@ref(fig:moderators-discernment) and \@ref(fig:moderators-bias) visualize those results by showing the distribution of effect sizes by moderator variable.

```{r moderators-discernment-table}
# display models
modelsummary::modelsummary(moderator_models_accuracy,
                           title = 'Moderator effects on Discernment',
                           #stars = TRUE,
                           coef_rename = c("country_groupedUS" = "Country: US (vs. nonUS)",
                                           "political_concordancediscordant" = "Political Concordance : Discordant (vs. Concordant)",
                                           "news_family_groupedother" = "News family: Other (vs. Covid)",
                                           "news_family_groupedpolitical" = "News family: Political (vs. Covid)",
                                           "news_format_groupedheadline_picture" = "News Format: Headline & Picture (vs. Headline)",
                                           "news_format_groupedheadline_picture_lede" = "News Format: Headline, Picture & Lede (vs. Headline)",
                                           "news_sourceTRUE" = "News source: Source (vs. No source)",
                                           "accuracy_scale_grouped6" = "Accuracy Scale: 6 (vs. 4)",
                                           "accuracy_scale_grouped7" = "Accuracy Scale: 7 (vs. 4)",
                                           "accuracy_scale_groupedbinary" = "Accuracy Scale: binary (vs. 4)",
                                           "accuracy_scale_groupedother" = "Accuracy Scale: other (vs. 4)",
                                           "perfect_symetryTRUE" = "Symmetrie: perfect (vs. imperfect)",
                                           "selection_fake_news_groupedidentified by researchers" = "False news: verified by researchers (vs. taken from fact check sites)"
                                           ), 
                           estimate = "{estimate}",
                           statistic = c("z = {statistic}",
                                         "p = {p.value}"),
                           output = "kableExtra"  # Ensure kableExtra is used
                           ) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down") %>%
  footnote(general = "Results of meta-regressions for different moderator variables. No adjustements have been made. Note that the last column for the model with all moderators does not display effect sizes for all moderator categories, because not all combinations of moderator categories are present in the data, case in which no effect size can be computed."
           , threeparttable = TRUE)
```

```{r moderators-bias-table}
# display models
modelsummary::modelsummary(moderator_models_error,
                           title = 'Moderator effects on Skepticism  bias',
                           #stars = TRUE,
                           coef_rename = c("country_groupedUS" = "Country: US (vs. nonUS)",
                                           "political_concordancediscordant" = "Political Concordance : Discordant (vs. Concordant)",
                                           "news_family_groupedother" = "News family: Other (vs. Covid)",
                                           "news_family_groupedpolitical" = "News family: Political (vs. Covid)",
                                           "news_format_groupedheadline_picture" = "News Format: Headline & Picture (vs. Headline)",
                                           "news_format_groupedheadline_picture_lede" = "News Format: Headline, Picture & Lede (vs. Headline)",
                                           "news_sourceTRUE" = "News source: Source (vs. No source)",
                                           "accuracy_scale_grouped6" = "Accuracy Scale: 6 (vs. 4)",
                                           "accuracy_scale_grouped7" = "Accuracy Scale: 7 (vs. 4)",
                                           "accuracy_scale_groupedbinary" = "Accuracy Scale: binary (vs. 4)",
                                           "accuracy_scale_groupedother" = "Accuracy Scale: other (vs. 4)",
                                           "perfect_symetryTRUE" = "Symmetrie: perfect (vs. imperfect)",
                                           "selection_fake_news_groupedidentified by researchers" = "False news: verified by researchers (vs. taken from fact check sites)"
                                           ), 
                           estimate = "{estimate}",
                           statistic = c("z = {statistic}",
                                         "p = {p.value}"),
                           output = "kableExtra"  # Ensure kableExtra is used
                           ) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down") %>%
  footnote(general = "Results of meta-regressions for different moderator variables. No adjustements have been made. Note that the last column for the model with all moderators does not display effect sizes for all moderator categories, because not all combinations of moderator categories are present in the data, case in which no effect size can be computed."
           , threeparttable = TRUE)
```

(ref:moderators-discernment) *Moderator effects on discernment*. The figure shows the distribution of effect sizes for discernment by moderator variables.

```{r moderators-discernment, fig.height= 10, fig.cap="(ref:moderators-discernment)"}
moderator_plots(accuracy_effect, "Discernment")
```

(ref:moderators-bias) *Moderator effects on skepticism bias*. The figure shows the distribution of effect sizes for skepticism bias by moderator variables.

```{r moderators-bias, fig.height= 10, fig.cap="(ref:moderators-bias)"}
moderator_plots(error_effect, "Skepticism  bias")
```

### Not preregistered moderators

#### Scale symmetry

First, to avoid biasing our estimate for H2, we removed one study [@baptistaInfluencePoliticalIdeology2021] that used a very asymmetrical set of answer options asked participants ("According to your knowledge, how do you rate the following headline? 1---not credible; 2---somehow credible; 3---quite credible; 4---credible; 5---very credible"). Second, we coded whether the remaining scales were perfectly symmetrical or not. Table \@ref(tab:n-symmetry) shows the frequency by which both scale types occurred.

```{r n-symmetry}
meta_wide %>% group_by(perfect_symetry) %>% summarise(Papers = n_distinct(paperID),
                                                     Samples = n_distinct(unique_sample_id),
                                                     Effects = n_distinct(observation_id)
                                                     ) %>% 
  mutate(perfect_symetry = ifelse(perfect_symetry == TRUE, "Perfect symmetry", 
                                 "Imperfect Symmetry")) %>% 
  pivot_longer(-perfect_symetry,
               names_to = "variable",
               values_to = "frequency"
               ) %>% 
  pivot_wider(names_from = perfect_symetry, 
              values_from = frequency) %>% 
  column_to_rownames(var = "variable") %>% 
  apa_table(caption = "Frequency table of scales")
```

Perfectly symmetrical scales include all binary scales (e.g. "True" or "False", "Real" or "Fake", is accurate "Yes" or "No", is accurate and unbiased "Yes" or "No"), and most Likert-scales (1 to 7: "Definitely fake" [1] to "Definitely real" [7], "Very unreliable" [1] to "Very reliable" [7], "Extremely unlikely" [1] to "Extremely likely" [7], "Extremely unbelievable" [1] to "Extremely believable" [7]; 1 to 6: "Extremely inaccurate" [1] to "Extremely accurate" [6], "Completely false" [1] to "Completely true" [6]). Yet, we coded the most common scale, a 4-point Likert scale ([1] not at all accurate, [2] not very accurate, [3] somewhat accurate, [4] very accurate), as not perfectly symmetrical. We coded two other Likert scales as not perfectly symmetrical ("not at all trustworthy" [1] to "very trustworthy" [10]; "not at all" [1] to "very" [7]).

Third, we investigated whether H1 and H2 hold for both perfectly symmetrical and imperfectly symmetrical scales. While both H1 and H2 hold for both symmetry types, we found that studies with perfectly symmetric scales tend to yield lower discernment scores ($\Delta$ Discernment = `r list_moderators_accuracy$Symmetrie$perfect_symetryTRUE$estimate` `r list_moderators_accuracy$Symmetrie$perfect_symetryTRUE$ci`) than studies relying on scales that are at least slightly asymmetric (Baseline discernment slightly asymmetric scales = `r list_moderators_accuracy$Symmetrie$intercept$estimate` `r list_moderators_accuracy$Symmetrie$intercept$ci`). Importantly, we do not find a difference regarding skepticism bias.

The results suggest that imperfectly symmetrical scales may inflate discernment. However, the symmetry of response scales was not a factor that was experimentally manipulated, and the studies we compare in our model differ in many other ways and the observed difference is likely confounded.

#### Proportion of true news

Most studies exposed participants to 50% false and 50% true news, whereas outside of experimental settings, people on average are exposed to much more true news than false news [@altayQuantifyingInfodemicPeople2022]. This inflated proportion of false news may increase discernment or make participants more skeptical of true news. Experimental evidence suggests that the ratio of false news has no effect on discernment and slightly increases skepticism in news judgment [@altayExposureHigherRates2023]. Figure \@ref(fig:share-true) shows effect sizes for discernment and skepticism bias as a function of news ratio. Due to the very uneven number of effect sizes, it does not seem reasonable to run a meta-regression to test this. However, Fig. \@ref(fig:share-true) suggests no obvious trend with regard to the share of true news ratio. Besides, as for the other moderator variables, any observed association is likely to be confounded by other factors.

(ref:share-true) *Effects of true-false ratio*. Effect sizes plotted by their share of true news among all news that an individual participant saw.

```{r share-true, fig.cap="(ref:share-true)"}
# for a plot with share as a factor variable, run this:
# share_true_discernment <- plot_share_true(data = accuracy_effect, name = "Discernment")
# share_true_bias <- plot_share_true(data = error_effect, name = "Skepticism  bias")

share_true_discernment <- plot_share_true_continuous(data = accuracy_effect, name = "Discernment")

share_true_bias <- plot_share_true_continuous(data = error_effect, name = "Skepticism  bias")

ggarrange(share_true_discernment, 
          share_true_bias  + 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank()))
```

#### Selection of false news

The majority of studies selected false news items from fact checking sites (e.g. Snopes). However, in some studies, veracity of news items has been established by researchers (or fact-checkers hired by researchers). Table \@ref(tab:non-fact-check-sites-studies) lists these studies. 

```{r non-fact-check-sites-studies}
studies <- meta_wide %>% 
  filter(selection_fake_news_grouped == "identified by researchers") %>% 
  distinct(ref, reference) %>% 
  mutate(id = 1:nrow(.)) %>% 
  select(id, reference) 

# the kable version puts the table into appendix J. Use the apa_table() alternative
# kbl(studies, longtable = T, 
#     #col.names = NULL, 
#     caption = "Studies that did not select false news items from fact-checking sites.") %>%
#   kable_paper(full_width = F) %>%
#   kableExtra::kable_styling(font_size = 8) %>%
#   column_spec(1, bold = T, border_right = T) %>%
#   column_spec(2, width = "35em") 


papaja::apa_table(
    studies, 
    caption = "Studies that did not select false news items from fact-checking sites.",
    col.names = c("", "Reference"),
    align = c("l", "m{16cm}"),
    font_size = "scriptsize"
    )
```

Three of these studies reduced researcher selection bias by automatically sampling news items [@garrettConservativesSusceptibilityPolitical2021; @aslettOnlineSearchesEvaluate2024; @allenScalingFactcheckingUsing2021]. We discuss these studies in detail in Appendix \@ref(selection-bias). Here, we rely on the slightly broader definition of news items not taken from fact-checking websites. As shown in Tables \@ref(tab:moderators-discernment-table) we find no difference in discernment when comparing studies that relied on news from fact-checking sites, compared to studies in which researchers established veracity of news items. We do find a difference regarding skepticism bias (see Table \@ref(tab:moderators-bias-table)), such that studies relying on false news items as verified by the researchers show reduced (to almost 0) skepticism bias, compared to studies relying on false news items as verified by fact-checking organizations. 

Note that, as with all between-study moderators, these estimates are likely confounded. The vast majority of effect sizes in the 'verified by researchers' category come from a single panel study [@garrettConservativesSusceptibilityPolitical2021]. This paper finds a negative skepticism bias for politically concordant news, suggesting that people are gullible towards information they politically approve. They did not find a skepticism bias for politically discordant items. Political concordance, therefore, is one reasonable candidate of a confounder for the observed difference regarding false news selection. However, as shown in Appendix \@ref(selection-bias), the (comparatively few) effect sizes from two other two studies relying on automated news selection also consistently yield a negative skepticism bias (i.e. gullibility bias). Automated news selection might therefor be a relevant factor, perhaps more important than merely not selecting news from fact-checked websites (as did the other studies in Table \@ref(tab:non-fact-check-sites-studies)). 

```{r}
# Code to check results from the Garrett study
# Garrett <- meta_wide %>% filter(ref == "Garrett_2021")
# 
# # calculate main effect sizes (Cohen's d, following Cochrane) 
# Garrett_accuracy_effect <- calculate_effect_sizes(effect = "accuracy", measure = "Cochrane", data = Garrett)
# Garrett_error_effect <- calculate_effect_sizes(effect = "error", measure = "Cochrane", data = Garrett)
# 
# # Models using Cohen's d
# Garrett_model_accuracy <- calculate_models(data=Garrett_accuracy_effect, robust = TRUE)
# Garrett_model_error <- calculate_models(data=Garrett_error_effect, robust = TRUE)
# 
# # Model for skepticism bias with concordance as moderator
# Garrett_error_concordance <-  metafor::rma.mv(yi, vi, 
#                                                 mods = ~political_concordance,
#                                                 random = ~ 1 | unique_sample_id/observation_id, 
#                                                 data=Garrett_error_effect)
# 
# summary(Garrett_model_accuracy)
# summary(Garrett_model_error)
# summary(Garrett_error_concordance)
```

\FloatBarrier
