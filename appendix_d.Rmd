# Binary vs. continuous scales {#binary}

\FloatBarrier  

Do people answer differently on binary scales than on non-binary scales? Our moderator analysis suggest that studies with binary scales yield both (i) lower discernment and (ii) less skepticism bias. In this section, we first check if we observe this difference more generally between all Likert scales (i.e. not only the 4-point scale used as reference in our moderator analysis), and binary scales. We find a statistically significant difference regarding discernment, but not regarding skepticism bias. As discussed in the moderator analysis, these observations might be confounded by all sorts of factors by which studies differ. Here, we focus on whether they could be the result of a measurement problem: What difference does it make to record responses on a binary scale, compared to a Likert scale? In a first step, to provide a test, we use a subset of studies we have individual-level data on, and collapse Likert scale response into dichotomous answers. For example,on a 4-point scale, we coded responses of 1 and 2 as not accurate (0) and 3 and 4 as accurate(1). For scales, with a mid-point (example 3 on a 5-point scale), we coded midpoint answersas 'NA'. We find a skepticism bias with the original Likert scale version (see also Appendix \@ref(individual-level)), but not with the dichotomous version. In a second step, we look at studies we have individual-level data on and which use binary answer scales. For these studies, we do find both positive discernment and positive skepticism bias, although smaller estimates than our overall meta-analytic averages. We replicate this finding when adding the dichotomized version of the likert scale studies from the first test. We further show that these results hold when using more appropriate summary statistics for binary outcomes, namely (log) odds ratios. Taken together, this suggests that skepticism bias stems partly from mis-classifications (the observed skepticism bias in binary response studies), but partly from degrees of confidence (the difference between likert-version and collapsed binary version). On average, people tend to (i) classify true news as false more often than false news as true and (ii) even when classifying equally well for both and true news, they rate true news as less extremely accurate than false news as inaccurate, suggesting lower confidence in their accuracy answers for true news. 

## Meta-regression

We ran a meta-regression using scale type (two levels: binary vs. continuous) as a predictor variable. Table \@ref(tab:binary-vs-continuous-regression) summarizes the results, and Fig. \@ref(fig:binary-continuous-descriptive) illustrates them. The analysis suggests that discernment (but not skepticism  bias) is more enhanced among continuous studies. 

```{r}
# Start with making a new variable distinguishing between binary and 
# continuous scales. 
add_binary_continuous_scales <- function(data) {
  
  results <- data %>% 
    mutate(scale_binary_continuous = ifelse(accuracy_scale == "binary", "binary", 
                                            "continuous")
    )
}

accuracy_effect <- add_binary_continuous_scales(accuracy_effect)
error_effect <- add_binary_continuous_scales(error_effect)
```

(ref:binary-continuous-descriptive) Distribution of effect sizes (Cohen's d) grouped by whether a binary or continuous (Likert-scale type) response scale was used.

```{r binary-continuous-descriptive, fig.cap="(ref:binary-continuous-descriptive)"}
data <- bind_rows(accuracy_effect %>% mutate(outcome = "accuracy"), 
                  error_effect %>% mutate(outcome = "error")) %>% 
  # Change outcome names
  mutate(outcome = ifelse(outcome == "accuracy", "Discernment", "Skepticism  bias"))

ggplot(data,
       aes(x = scale_binary_continuous, y = yi)) +
    geom_half_boxplot(aes(x = scale_binary_continuous, color = scale_binary_continuous), side = "l", size = 0.5, nudge = 0.05, 
                      outlier.shape = NA) +
    geom_half_violin(aes(fill = scale_binary_continuous), side = "r") +
    geom_half_point(aes(color = scale_binary_continuous), side = "l", 
                    transformation_params = list(height = 0, width = 0.1, seed = 1)) +
    # add line of 0
    geom_hline(yintercept = 0, 
               linewidth = 0.5, linetype = "24", color = "grey") +
    # colors 
    scale_color_viridis_d(option = "plasma", end = 0.9)+
    scale_fill_viridis_d(option = "plasma", end = 0.9) +
    # labels and scales
    labs(x = NULL, y = "Cohen's d", fill = NULL, 
         title = "Binary vs. Likert Scales") +
  guides(color = FALSE, fill = FALSE) +
    plot_theme +
    coord_flip() +
  plot_theme +
    theme(strip.text = element_text(size = 14)) +
    facet_wrap(~outcome)
```

```{r}
# function that calculates a meta regression and takes a data frame
# as input
scale_comparison_meta_regression <- function (data, return_as = "model") {
  
  model <- robust(metafor::rma.mv(yi, vi, 
                                  mods = ~scale_binary_continuous,
                                  random = ~ 1 | unique_sample_id / 
                                    observation_id, data = data),
                  cluster = data$unique_sample_id
  ) 
  
  if (return_as == "model") {
    return(model)
  }
  
  if(return_as == "tidy") {
    
    model <- model %>% 
      tidy(conf.int = TRUE) %>% 
      mutate(model = "Original SMDs (all data)") %>% 
      # give a nicer name to the estimate
      mutate(term = ifelse(term == "scale_binary_continuouscontinuous", 
                           "effect of continuous scale (baseline binary)", term)
      )
    
    return(model) 
  }
}

# calculate the model for accuracy
# scales_comparison_all_data_accuracy <- scale_comparison_meta_regression(accuracy_effect, return_as = "tidy")
# # for error
# scales_comparison_all_data_error <- scale_comparison_meta_regression(error_effect, return_as = "tidy")

# discernment
scales_comparison_discernment <- scale_comparison_meta_regression(accuracy_effect)

# skepticism  bias
scales_comparison_bias <- scale_comparison_meta_regression(error_effect)
```

```{r binary-vs-continuous-regression}
# main result table
modelsummary::modelsummary(list("Discernment" = scales_comparison_discernment, 
                                "Skepticism  bias" = scales_comparison_bias),
                           title = 'Binary vs. Likert Scales', 
                           #stars = TRUE, 
                           coef_rename = c("scale_binary_continuouscontinuous" = "Continuous (vs. binary)"), 
                           estimate = "{estimate}",
                           statistic = c("z = {statistic}",
                                         "p = {p.value}"),
                           output = "kableExtra"  # Ensure kableExtra is used
                           ) %>%
  footnote(general = "Results of a meta-regression using scale type (two levels: binary vs. continuous) as the moderator variable. No adjustements have been made."
           , threeparttable = TRUE)
```

## Dichotomizing likert scale responses

To further investigate the effect of scale type, we run a test on a subset of studies that we have individual-level data on and that used Likert scales. For this subset, we made two versions: (i) a version with the original Likert scale scores; (ii) a dichotomized version where we collapsed the Likert scale scores into either 'false' or 'true'. For example, on a 4-point scale, we coded responses of 1 and 2 as not accurate (0) and 3 and 4 as accurate(1). For scales, with a mid-point (example 3 on a 5-point scale), we coded midpoint answers as 'NA'. 

We calculate the summary statistics for both versions and run the same meta-analytic models on that subset. Table \@ref(tab:table-individual-dichotomized) summarizes the results. 

```{r}
# We will make two version of individual level data. The first uses the original likert scale responses. The second takes all individual level data and computes binary responses for those studies that used a continuous scale. For example, on a 4-point scale, we would code 1 and 2 as not accurate (0) and 3 and 4 as accurate (1). For scales, with a mid-point (example 3 on a 5-point scale), we will code midpoint answers as NA.

# load data
individual_level_subset <- read_csv("data/individual_level_subset.csv") %>% 
  # remove binary scales and treatment conditions
  filter(scale != "binary" & condition == "control")   %>% 
  # remove NA's for accuracy
  drop_na(accuracy) %>% 
    # remove NA's for veracity 
  # (one study, Allen_2021, has non-established veracity for some items)
  drop_na(veracity)
  

# original likert scale responses 
individual_level_subset <- individual_level_subset %>% 
  # identify unique samples
  mutate(unique_sample_id = paste(paper_id, sample_id, sep = "_"),
         # make an ungrouped versions just for comparison
         ungrouped_std_acc = accuracy/sd(accuracy), 
         ungrouped_std_err = error/sd(accuracy),
         # make a unique id for all participants
         unique_subject_id = paste0(unique_sample_id, id)
         ) 

# dichotomized responses
individual_level_subset_binary <- individual_level_subset %>% 
# transform accuracy scores to binary
  mutate(
    # add helper variable that indicates scales with midpoint
    midpoint_scale = ifelse(scale_numeric %% 2 != 0, TRUE, FALSE),
    accuracy = case_when(
      # code midpoints as NA,
      midpoint_scale == TRUE & 
        accuracy == (scale_numeric/2)+0.5 ~ NA,
      # transform continuous scores
      accuracy <= scale_numeric/2 ~ 0, 
      accuracy > scale_numeric/2 ~ 1, 
      TRUE ~ NA),
    # calculate error
    error = ifelse(veracity == "true", 
                   # for fake news "accurate" is the wrong response
                   1-accuracy, 
                   # for true news "accurate" is correct response
                   accuracy)
  )


# check
# test <- individual_level_subset %>% 
# # transform accuracy scores to binary
#   mutate(
#     # add helper variable that indicates scales with midpoint
#     midpoint_scale = ifelse(scale_numeric %% 2 != 0, TRUE, FALSE),
#     accuracy_binary = case_when(
#       # code midpoints as NA,
#       midpoint_scale == TRUE & 
#         accuracy == (scale_numeric/2)+0.5 ~ NA,
#       # transform continuous scores
#       accuracy <= scale_numeric/2 ~ 0, 
#       accuracy > scale_numeric/2 ~ 1, 
#       TRUE ~ NA),
#     # calculate error
#     error_binary = ifelse(veracity == "true", 
#                    # for fake news "accurate" is the wrong response
#                    1-accuracy_binary, 
#                    # for true news "accurate" is correct response
#                    accuracy_binary)
#   ) %>% 
#   select(unique_subject_id, veracity, scale, contains("accuracy"), contains("error"))
```

```{r}
# make summarized meta data like version
to_meta_data <- function(data){
  
  # calculate the average correlation between fake news accuracy ratings and true news accuracy ratings for each sample.
correlations_by_sample <- data %>%
  # step 1: for each participant, calculate means of fake and true
  group_by(paper_id, sample_id, id, veracity, condition, scale) %>% 
  summarize(mean_accuracy = mean(accuracy)) %>% 
  pivot_wider(names_from = veracity, values_from = mean_accuracy, names_prefix = "accuracy_") %>% 
  # there is one NA value in Bago_2022 that we need to remove
  drop_na(accuracy_true, accuracy_fake) %>% 
  # step 2: for each sample, calculate correlations of means of fake and true
  group_by(paper_id, sample_id, condition, scale) %>% 
  summarize(r = cor(accuracy_fake, accuracy_true)) 

# step 3: get average by-sample correlation
average_correlation_by_sample <- correlations_by_sample %>% 
  ungroup() %>% 
  summarize(average_r = mean(r, na.rm=TRUE)) %>% 
  pull(average_r)
average_correlation_by_sample

# extract number of observations

n_observations_data <- data %>% 
   group_by(paper_id, sample_id, unique_sample_id) %>%
  summarize(n_observations = n())
  
# make meta data
  meta_data <- data %>%
  group_by(paper_id, sample_id, unique_sample_id, veracity) %>%
  drop_na(veracity) %>% 
  summarise(across(c(accuracy, error), list(mean = ~mean(., na.rm = TRUE),
                                            sd = ~sd(., na.rm = TRUE)
                                            ),
                   # to make compatible with names used in effect size computation functions
                   .names = "{fn}_{col}"
                   )
            ) %>% 
  pivot_wider(names_from = veracity, 
              values_from = c(starts_with("mean"), starts_with("sd"))) %>% 
  # to make compatible with names used in effect size computation functions
  rename(error_true = mean_error_true, 
         error_fake = mean_error_fake) %>% 
    # add variable necessary for effect size computation 
    ungroup() %>% 
    mutate(design = "within", 
           cor = average_correlation_by_sample, 
           observation_id = 1:nrow(.))
  
  meta_data <- left_join(meta_data, n_observations_data)
  
  return(meta_data)
    
}

# summarize data to meta-analysis format
likert_summarized <- to_meta_data(individual_level_subset)

binary_summarized <- to_meta_data(individual_level_subset_binary)
```

```{r}
# calculate effect sizes 

# likert version
likert_accuracy_effect <- calculate_effect_sizes(effect = "accuracy", measure = "Cochrane", data = likert_summarized)

likert_error_effect <- calculate_effect_sizes(effect = "error", measure = "Cochrane", data = likert_summarized)

# dichotomized version
binary_accuracy_effect <- calculate_effect_sizes(effect = "accuracy", measure = "Cochrane", data = binary_summarized)

binary_error_effect <- calculate_effect_sizes(effect = "error", measure = "Cochrane", data = binary_summarized)

# Models using Cohen's d

# likert version
likert_accuracy_model <- calculate_models(data=likert_accuracy_effect, robust = TRUE)
likert_error_model <- calculate_models(data=likert_error_effect, robust = TRUE)

# dichotomized version
binary_accuracy_model <- calculate_models(data=binary_accuracy_effect, robust = TRUE)
binary_error_model <- calculate_models(data=binary_error_effect, robust = TRUE)
```

```{r table-individual-dichotomized}
# Results table
modelsummary::modelsummary(list("Discernment" = likert_accuracy_model, 
                                "Skepticism  bias" = likert_error_model,
                                "Discernment" = binary_accuracy_model, 
                                "Skepticism  bias" = binary_error_model),
                           title = "Original Likert-scale vs. dichotomized version", 
                           #stars = TRUE, 
                           coef_rename = c("overall" = "Estimate"), 
                           estimate = "{estimate}",
                           statistic = c("z = {statistic}",
                                         "p = {p.value}"),
                           output = "kableExtra"  # Ensure kableExtra is used
                           ) %>%
  add_header_above(c(" " = 1, "Original Likert scale" = 2, "Dichotomized" = 2)) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down") %>%
  footnote(general = "Meta-analyses following the modeles of the main paper, on the subset of studies that we have individual-level data on and that used Likert scales. For this subset, we made two versions: (i) a version with the original Likert scale scores; (ii) a dichotomized version where we collapsed the Likert scale scores into either 'false' or 'true'. For example, on a 4-point scale, we coded responses of 1 and 2 as not accurate (0) and 3 and 4 as accurate(1). For scales, with a mid-point (example 3 on a 5-point scale), we coded midpoint answers as 'NA'. No adjustments have been made."
           , threeparttable = TRUE)


```




## Binary response scales

Above, we found that skepticism bias disappears when dichotomizing scales of studies intially recording responses on Likert scales. How about studies who recorded responses on a binary scale? Here, we focus on the subset of studies that we have raw, individual-level data on, and focus on the studies using a binary response scale.

In addition the the main meta-analytic models, here, we additionally present results based on more appropriate effect sizes for binary data, namely log odds ratios (logORs). In our main analysis, we combined studies which measure perceived accuracy on a continuous scale, and studies who do so on a binary scale. This is not problematic per se - there are [statistical methods to compare effects on both scales](https://training.cochrane.org/handbook/current/chapter-10#section-10-6) [@higgins_cochrane_2019]. These require, however, appropriate summary statistics for both scales. For continuous measures, means and standard deviations are fine; for binary measures we would need, for example, odds or risk ratios. The problem we were facing is that authors did not provide the appropriate summary statistics for binary scales. Instead, they tended to report means and standard deviations, just as they do for continuous outcomes. For the main analysis, we made the decision to treat continuous and binary scales in the same way, glossing over potential biases from inappropriate summary statistics. 

```{r, message=FALSE}
# Step 1: extract `unique_sample_id` for samples with binary outcomes that 
# raw data is available for 

# load data (again, since we previously did some modifications)
individual_level_subset <- read_csv("data/individual_level_subset.csv") %>%  # remove binary scales and treatment conditions
  filter(condition == "control")   %>% 
  # remove NA's for accuracy
  drop_na(accuracy) %>% 
    # remove NA's for veracity 
  # (one study, Allen_2021, has non-established veracity for some items)
  drop_na(veracity) 

binary_samples_raw_data <- individual_level_subset %>% 
  filter(scale == "binary") %>% 
  # (note: do not pick `paper_id` because within a paper, some samples might have
  # been measured on binary, others on a continuous scale)
  summarize(unique(unique_sample_id)) %>% 
  pull()

# Step 2: filter the meta data frame
# We want to reduce our data to continuous measure samples AND only those binary 
# measure samples that we have raw data on

reduce_binary_samples <- function(data) {
  
    results <- data %>% 
      filter(accuracy_scale != "binary" | unique_sample_id %in% all_of(binary_samples_raw_data))
}
# for accuracy
reduced_accuracy_effect <- reduce_binary_samples(accuracy_effect)
# for error
reduced_error_effect <- reduce_binary_samples(error_effect)

```

### Odds ratios 

We first calculated the odds ratios from the raw data^[A general overview of appropriate summary statistics for binary outcomes can be found here(@higgins_cochrane_2019): https://training.cochrane.org/handbook/current/chapter-06#section-6-4)]. The 'odds' refer to the ratio of the probability that a particular event will occur to the probability that it will not occur, and can be any number between zero and infinity [@higgins_cochrane_2019]. It is commonly expressed as a ratio of two integers. For example, in a clinical context, 1 out of 100 patients might die; then the odds of dying are `0.01`, or `1:100`.

The odds *ratio* (OR) is the ratio of the Odds. The odds ratio that characterizes discernment is calculated as

$$
OR_{Accuracy} = \frac{(Accurate_{true}/ NotAccurate_{true})}{(Accurate_{false}/ NotAccurate_{false})}
$$

If the OR is `1`, participants were just as likely to rate items as 'accurate' when looking at true news as they were when looking at false news. If the OR is `> 1`, then participants rated true news as more accurate than fake news. An OR of `2` means that participants were twice as likely to rate true news as accurate compared to false news.

The OR for skepticism  bias is calculated as

$$
OR_{Error} = \frac{(NotAccurate_{true}/Accurate_{true})}{(Accurate_{false}/NotAccurate_{false})} \leavevmode \newline
= \frac{\frac{1}{(NotAccurate_{true}/Accurate_{true})}}{(Accurate_{false}/NotAccurate_{false})} \leavevmode \newline
= \frac{1}{OR_{Accuracy}}
$$
For our analysis, we calculated the odds ratio (OR) for both accuracy and error. More precisely, we expressed the OR on a logarithmic scale, also referred to as "log odds ratio"(logOR). As for odds ratios, if the log odds ratio is positive, it indicates positive discernment/skepticism  bias^[To interpret the magnitude of that difference we have to transform the logarithmic estimate back to a normal odds ratio. The reason we use the log odds ratios in the first place is that which makes outcome measures symmetric around 0 and results in corresponding sampling distributions that are closer to normality [@viechtbauer_conducting_2010] ].

Table \@ref(tab:frequency) shows the frequency of answers by veracity.

```{r}
# We will make two version of individual level data. The main one only considers studies that actually used a binary response scale (`scale == binary`). The second takes all individual level data and computes binary responses for those studies that used a continuous scale. For example, on a 4-point scale, we would code 1 and 2 as not accurate (0) and 3 and 4 as accurate (1). For scales, with a mid-point (example 3 on a 5-point scale), we will code midpoint answers as NA.

# load individual level data
# load data
individual_level_subset <- read_csv("data/individual_level_subset.csv") %>% 
  # remove binary scales and treatment conditions
  filter(condition == "control")   %>% 
  # remove NA's for accuracy
  drop_na(accuracy) %>% 
    # remove NA's for veracity 
  # (one study, Allen_2021, has non-established veracity for some items)
  drop_na(veracity) %>%  
# transform accuracy scores to binary
  mutate(
    # add helper variable that indicates scales with midpoint
    midpoint_scale = ifelse(scale_numeric %% 2 != 0, TRUE, FALSE),
    accuracy = case_when(
      scale == "binary" ~ accuracy,
      # code midpoints as NA,
      midpoint_scale == TRUE & 
        accuracy == (scale_numeric/2)+0.5 ~ NA,
      # transform continuous scores
      accuracy <= scale_numeric/2 ~ 0, 
      accuracy > scale_numeric/2 ~ 1, 
      TRUE ~ NA),
    # calculate error
    error = ifelse(veracity == "true", 
                   # for fake news "accurate" is the wrong response
                   1-accuracy, 
                   # for true news "accurate" is correct response
                   accuracy)
  )

# make descriptive summary data for binary studies only 
descriptive_binary <- individual_level_subset %>%  
  filter(scale == "binary") %>% 
  group_by(veracity) %>% 
  summarize(
    sum_accuracy = sum(accuracy, na.rm = TRUE), 
    sum_NO_accuracy = sum(1 - accuracy, na.rm = TRUE)
  ) %>%
  rowwise() %>%
  mutate(
    total_sum = sum(sum_accuracy, sum_NO_accuracy),
    rel_accuracy = sum_accuracy / sum(sum_accuracy, sum_NO_accuracy),
    rel_NO_accuracy = sum_NO_accuracy / sum(sum_accuracy, sum_NO_accuracy), 
    total_rel = sum(rel_accuracy + rel_NO_accuracy)
  ) %>%
  ungroup() %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  # add brackets around relative values
  mutate(across(c(rel_accuracy, rel_NO_accuracy, total_rel), ~paste0("(", ., ")"))) %>% 
      # unite absolute and relative values in the same variable
  mutate(accurate = paste0(sum_accuracy, " ", rel_accuracy), 
         not_accurate = paste0(sum_NO_accuracy, " ", rel_NO_accuracy),
         total = paste0(total_sum, " ", total_rel)
         ) %>% 
  select(veracity, accurate, not_accurate, total)
```

```{r frequency}
# define column names for a nicer table
new_names <- c("Veracity", "Rated as accurate", "Rated as not accurate", "Sum")
colnames(descriptive_binary) <- new_names

# summary table
apa_table(descriptive_binary , 
      note = "Frequency of responses (among individual-level studies with binary response scales)") 

```

```{r}
# Make a log odds version
calculate_OR <- function(data, outcome) {
  
  # make a data frame that `escalc` can use to calculate logOR
  odds_ratios <- data %>%  
    group_by(unique_sample_id, veracity) %>% 
    summarize(
      sum_accuracy = sum(accuracy, na.rm = TRUE), 
      sum_NO_accuracy = sum(1-accuracy, na.rm = TRUE)
    ) %>% 
    pivot_wider(names_from = veracity, values_from = c(sum_accuracy, sum_NO_accuracy))
  
  if (outcome == "accuracy") {
  # calculate logOR using `escalc` for accuracy
  odds_ratios_accuracy <- escalc(measure="OR", 
                                 # true / fake
                                 ai= sum_accuracy_true, 
                                 bi=sum_NO_accuracy_true, 
                                 ci=sum_accuracy_fake,
                                 di=sum_NO_accuracy_fake,
                                 data = odds_ratios ) %>% 
    # add observation_id variable
    mutate(observation_id = 1:nrow(.)) 
  
  return(odds_ratios_accuracy)
  }
  
  if (outcome == "error") {
  # calculate logOR using `escalc` for error
  odds_ratios_error <- escalc(measure="OR", 
                              # true / fake
                              bi= sum_accuracy_true, 
                              ai=sum_NO_accuracy_true, 
                              ci=sum_accuracy_fake,
                              di=sum_NO_accuracy_fake,
                              data = odds_ratios ) %>% 
    # add observation_id variable
    mutate(observation_id = 1:nrow(.))
  
  return(odds_ratios_error)
  }
}
```

```{r}
# odds ratios for studies with binary response scales only
odds_ratios_accuracy <- calculate_OR(individual_level_subset %>% 
                                       filter(scale == "binary"), 
                                     outcome = "accuracy")
odds_ratios_error <- calculate_OR(individual_level_subset %>% 
                                    filter(scale == "binary"), 
                                  outcome = "error")

# odds ratios for all studies transposed to binary scale
odds_ratios_accuracy_all <- calculate_OR(individual_level_subset, 
                                     outcome = "accuracy")
odds_ratios_error_all <- calculate_OR(individual_level_subset, 
                                  outcome = "error")
```

### Meta-analyses 

We ran two meta-analyses on two different data sets: The first data set are only studies using binary response scales. Results are displayed in Table \@ref(tab:meta-odds). For reference, we also report a non-standardized estimator that likewise accounts for dependence between false and true news, namely the mean change (MC)^[We use the term mean change in line with vocabulary used by the metafor package and its `escalc()` function that we use for all effect size calculations. It is in fact a simple mean difference but one that accounts for the correlation between true and false news in the calculation of the standard error (see @higgins_cochrane_2019). Here is a direct link to the relevant chapter online: https://training.cochrane.org/handbook/current/chapter-23#section-23-2-7-1]. The second data set on all studies, with ratings of those studies originally using Likert-scale responses collapsed to binary outcomes (results in Table \@ref(tab:meta-odds-extended)). In both analyses, we find (i) positive discernment and (ii) positive response bias, using both the same Cohen's d effect sizes of our main analysis and effect sizes expressed in log Odds Ratios. Note, however, that these estimates are smaller than the our overall meta-analytic averages.

```{r}
compare_OR <- function(subset = "binary"){
  
  ##### SMCC (original meta analysis)
  
  # identify subset of individual_level_subset
  
  if(subset == "binary") {
    binary_individual_level_subset <- individual_level_subset %>% 
      filter(scale == "binary") %>% 
      reframe(unique(unique_sample_id)) %>% pull() 
  } else {
    binary_individual_level_subset <- individual_level_subset %>% 
      reframe(unique(unique_sample_id)) %>% pull() 
  }
  
  # restrict effect data frame to those binary studies
  reduce_binary_samples <- function(data) {
    
    results <- data %>% 
      filter(unique_sample_id %in% all_of(binary_individual_level_subset))
  }
  # get subset of relevant effect sizes
  binary_individual_accuracy_effect <- reduce_binary_samples(accuracy_effect)
  binary_individual_error_effect <- reduce_binary_samples(error_effect)
  
  # run model
  model_Cohensd_accuracy <- calculate_models(data=binary_individual_accuracy_effect, 
                                          robust = FALSE)
  model_Cohensd_error <- calculate_models(data=binary_individual_error_effect, 
                                       robust = FALSE)
  ##### Mean Difference (MD)
  
  # calculate (non-standardized) mean differences/change
  effect_MD_accuracy <- calculate_effect_sizes(effect = "accuracy", 
                                               measure = "MC",
                                               measure_between = "MD",
                                               data = meta_wide)
  effect_MD_error <- calculate_effect_sizes(effect = "error", 
                                            measure = "MC",
                                            measure_between = "MD",
                                            data = meta_wide)
  
  # get subset of relevant effect sizes
  binary_individual_accuracy_effect <- reduce_binary_samples(effect_MD_accuracy)
  binary_individual_error_effect <- reduce_binary_samples(effect_MD_error)
  
  
  # run model
  model_MD_accuracy <- calculate_models(data=binary_individual_accuracy_effect, 
                                        robust = FALSE)
  model_MD_error <- calculate_models(data=binary_individual_error_effect, 
                                     robust = FALSE)
  
  ##### OR
  
  if(subset == "binary") {
    
    # calculate models
    model_OR_accuracy <- calculate_models(data=odds_ratios_accuracy, robust = TRUE)
    model_OR_error <- calculate_models(data=odds_ratios_error, robust = TRUE)
  } else {
    # calculate models
    model_OR_accuracy <- calculate_models(data=odds_ratios_accuracy_all, robust = TRUE)
    model_OR_error <- calculate_models(data=odds_ratios_error_all, robust = TRUE)
  }
  
  return(list("Accuracy" = model_OR_accuracy, 
              "Error" = model_OR_error, 
              "Accuracy" = model_Cohensd_accuracy , 
              "Error" = model_Cohensd_error, 
              "Accuracy" = model_MD_accuracy , 
              "Error" = model_MD_error)
  )
}
```

```{r meta-odds}
comparison_binary <- compare_OR(subset = "binary") # change to subset = "all" for all individual-level studies collapsed to binary scale

# extract OR models
model_OR_accuracy <- comparison_binary[[1]]
model_OR_error <- comparison_binary[[2]]

modelsummary::modelsummary(comparison_binary,
                           title = 'Individual-level studies with binary response scale', 
                           #stars = TRUE, 
                           coef_rename = c("overall" = "Estimate"), 
                           estimate = "{estimate}",
                           statistic = c("z = {statistic}",
                                         "p = {p.value}"),
                           output = "kableExtra"  # Ensure kableExtra is used
) %>% 
  add_header_above(c(" " = 1,
                     "Log OR" = 2,
                     "Cohen's d" = 2,
                     "Mean change" = 2),
                   ) %>%
  add_header_above(c(" " = 1, "(based on individual data)" = 2, "(based on meta data)"= 4),
                   line = FALSE, italic = TRUE) %>%
  footnote(general = "Results of a meta-analyses using different effect-size estimators, on the subset of studies that we have individual-level data on and that use binary response scales.  Note that the number of observations differ, because some samples provide several effect sizes in the meta-data. For the odds ratios based on the individual data, however, we calculated only one average effect size per sample. No adjustments have been made.",
           threeparttable = TRUE) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down")
```

```{r}
# All models that use Cohen's D, above, are based on the Likert responses meta-data. 
# Make a Cohen's D version for binary and dichotomized effect sizes

# dichotomized and binary effect sizes
binary_summarized <- to_meta_data(individual_level_subset)

binary_accuracy_effect <- calculate_effect_sizes(effect = "accuracy", measure = "Cochrane", data = binary_summarized)
binary_error_effect <- calculate_effect_sizes(effect = "error", measure = "Cochrane", data = binary_summarized)

# models
binary_accuracy_model <- calculate_models(data=binary_accuracy_effect, robust = TRUE)
binary_error_model <- calculate_models(data=binary_error_effect, robust = TRUE)
```

```{r meta-odds-extended}
comparison_binary <- compare_OR(subset = "all") 

# extract OR models
model_OR_accuracy <- comparison_binary[[1]]
model_OR_error <- comparison_binary[[2]]

modelsummary::modelsummary(list(
  "Discernment" = model_OR_accuracy, 
  "Skepticism bias" = model_OR_error , 
  "Discernment" = binary_accuracy_model,
  "Skepticism bias" = binary_error_model
  
),
title = 'Individual-level studies with binary response scales and Likert scale ratings collapsed to binary responses', 
#stars = TRUE, 
coef_rename = c("overall" = "Estimate"),
escape = TRUE, 
estimate = "{estimate}",
statistic = c("z = {statistic}",
              "p = {p.value}"),
output = "kableExtra"  # Ensure kableExtra is used
) %>% 
  add_header_above(c(" " = 1,
                     "Log OR" = 2,
                     "Cohen's d" = 2),
  ) %>%
  footnote(general = "An extension to the previous table, where in addition to the studies that used a binary response scale, we added a dichotomized response version for studies that used Likert scales.", threeparttable = TRUE) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down")
```


