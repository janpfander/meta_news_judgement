
<!-- README.md is generated from README.Rmd. Please edit that file -->

# Spotting False News and Doubting True News, A Meta-Analysis of News Judgments

------------------------------------------------------------------------

[![OSF
Registration](https://img.shields.io/badge/OSF_Registration-10.17605%2FOSF.IO%2FSVC7U-blue)](https://doi.org/10.17605/OSF.IO/SVC7U)
[![OSF
Project](https://img.shields.io/badge/OSF_Project-10.17605/OSF.IO/96ZBP-blue)](https://osf.io/96zbp/)

**All this project’s materials are free and open**.

- [Replicate the findings](#replicate)
- [Check out the files glossary for this repository](#glossary)

![Open data](images/data_large_color.png)   ![Open
materials](images/materials_large_color.png)  
![Preregistration](images/preregistered_large_color.png)

------------------------------------------------------------------------

## Abstract

How good are people at judging the veracity of news? We conducted a
systematic literature review and pre-registered meta-analysis of 303
effect sizes from 67 experimental articles evaluating accuracy ratings
of true and fact-checked false news ($N_{participants}$ = 194’438 from
40 countries across 6 continents). We found that people rated true news
as more accurate than false news (Cohen’s d = 1.12 \[1.01, 1.22\], p \<
.001) and were better at rating false news as false than at rating true
news as true (Cohen’s d = 0.32 \[0.24, 0.39\], p \< .001). In other
words, participants were able to discern true from false news, and erred
on the side of skepticism rather than credulity. We found no evidence
that the political concordance of the news had an effect on discernment,
but participants were more skeptical of politically discordant news
(Cohen’s d = 0.78 \[0.62, 0.94\], p \< .001). These findings lend
support to crowdsourced fact-checking initiatives, and suggest that, to
improve discernment, there is more room to increase the acceptance of
true news than to reduce the acceptance of fact-checked false news.

------------------------------------------------------------------------

This repository contains the code and data for our paper.

> Jan Pfänder and Sacha Altay. 2025. “Spotting False News and Doubting
> True News, A Meta-Analysis of News Judgments”“. *Nature Human
> Behaviour* (forthcoming). Pre-print at
> <https://doi.org/10.31219/osf.io/n9h4y>

## Replicate

To maximize applicability, we wrote our manuscript using in RMarkdown,
allowing us to mix code, figures, text, and tables with the actual prose
of the manuscript. All results are programmatically included when
rendering the document. This allows to clearly see which code and data
generated which result.

To reproduce the findings and re-run the analysis, do the following:

1.  Download this repository. You can use GitHub to clone or fork the
    repository (see the green “Clone or download” button at the top of
    the GitHub page).
2.  Open `meta_news_judgement.Rproj` to open an [RStudio
    Project](https://r4ds.had.co.nz/workflow-projects.html).
3.  Within the RStudio Project, open the `preprint.Rmd` file and run it

For inspecting scripts directly on github, use the `.md` files. The
source code can be found in the `.Rmd` and `.R` files.

Please find the [session info below](#session-info).

*Estimated installation time* : If you have R and Rstudio installed,
then adding additional packages required in our R scripts is quick (a
couple of minutes)

*Estimated run time*: Rendering the `anonymized.Rmd` document takes
about 5mins on our machine (see [session info](#session-info))

## Files glossary

- The `manuscript.Rmd` and the `preprint.Rmd` generate the paper. The
  difference is that the `manuscript.Rmd` produces a version for
  submission, with a separate figure section, while the other generates
  the preprint version.

- The `codebook.csv` contains all variables of our data set.

- The `data/` folder contains the main data sets.

  - `data/data.csv` is the raw, hand-coded data that we extracted from
    the original papers (which is not yet accessible since it contains
    data not only for control, but also treatment conditions, which are
    relevant for another, unfinished project.)
  - `data/cleaned.csv` is a cleaned version of that raw data, where we
    change some variable names, recode values, and remove treatment
    conditions. The cleaning process is documented in the `cleaning.Rmd`
    file (but cannot be replicated at the moment since `data.csv` is not
    yet public).
  - `data/individual_level_subset.csv` and
    `data/correlations_by_sample.csv` are generated by the
    `compute_correlation.R` script. The first contains individual level
    data from a subset of studies for which we calculated the summary
    statistics ourselves. The second contains the correlation values for
    true and false news ratings by samples for that subset. We use these
    values to obtain the average by-sample correlation to calculate our
    effect sizes.
  - `data/ne_110m_admin_0_countries/` contains shape files from the
    Natural Earth project. We use the [“Admin 0 - Countries” 1:110m
    cultural
    shapefiles](https://www.naturalearthdata.com/downloads/110m-cultural-vectors/)
    for maps.

- The `functions/` folder contains functions that we rely upon in our
  analysis scripts. To keep the `anonymized.Rmd` document neat and
  readable, we wrote all lengthy code bits as external functions.

- The `literature_search/` folder documents our systematic literature
  reviews

  - `scopus.csv` and `google.csv` contain all results that popped up in
    our two literature searches.
  - `inclusion_criteria.csv` contains a list of our inclusion criteria
  - `prisma.csv` is generated by `prisma.Rmd` and contains all the
    relevant information for filling out the PRISMA flowchart (which we
    did by hand in a [template dowloaded
    here](http://www.prisma-statement.org/PRISMAStatement/FlowDiagram))
  - files preceded by `revisions_` are related to the second systematic
    review during the revisions process of the paper. They include
    search results, search strings, screening decisions of the two
    authors. The screening process is document in
    `revisions_screening_process.Rmd`

- The `data_extraction.R` file contains the code to obtain summary
  statistics from raw data studies. Once we calculated these summary
  statistics, we entered them by hand into the the raw data spreadsheet
  (`data/data.csv`). We do not publish the raw data from those studies
  here, since they are already publicly available.

- Files preceded by `appendix_*` contain appendices to the
  `anonymized.Rmd`. We stored them in separate documents to keep the
  `anonymized.Rmd` document as short as possible.

- Files preceded by `simulation_*` contain data scripts for generating
  and analyzing simulated data. We simulated data to help us clarify our
  estimands and choose estimators before writing our pre-registration.

- `preregistration.Rmd` contains the source file for our
  pre-registration which can be found on the
  [OSF](https://doi.org/10.17605/OSF.IO/SVC7U).

- `bibliography.bib` contains all references and `nature.csl` the
  citation formatting.

- `images/` contains images used for this README file.

## Session info

    #> R version 4.4.1 (2024-06-14)
    #> Platform: aarch64-apple-darwin20
    #> Running under: macOS 15.0
    #> 
    #> Matrix products: default
    #> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib 
    #> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0
    #> 
    #> locale:
    #> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
    #> 
    #> time zone: Europe/Paris
    #> tzcode source: internal
    #> 
    #> attached base packages:
    #> [1] stats     graphics  grDevices utils     datasets  methods   base     
    #> 
    #> loaded via a namespace (and not attached):
    #>  [1] compiler_4.4.1    here_1.0.1        fastmap_1.2.0     rprojroot_2.0.4  
    #>  [5] cli_3.6.3         tools_4.4.1       htmltools_0.5.8.1 rstudioapi_0.17.1
    #>  [9] yaml_2.3.10       rmarkdown_2.28    knitr_1.48        xfun_0.49        
    #> [13] digest_0.6.37     rlang_1.1.4       evaluate_1.0.1
